import { GoogleGenAI, Modality, Type } from "@google/genai";
import type { Message, LogAnalysis, PetState, EmotionSet, ApiKeys, Model, PetPersona } from '../types';
import { LEVEL_NAMES } from '../constants';
import { buildImagePrompt, buildExpressionPrompt, buildEventPrompt } from '../imagePrompts';
import { imageCache } from '../utils/imageCache';
import { conversationCache } from '../utils/conversationCache';
import { petSkinGenerator, skinSettings, type SkinTheme } from '../utils/petSkins';
import { trackAPICall } from '../components/PerformanceMonitor';
import { 
  buildSystemPrompt, 
  buildReflectionPrompt,
  buildPersonaSummaryPrompt,
  calculateAverageEmotions,
  getRecentLogs,
  buildRecentContext
} from '../utils/personaManager';

// FIX: Updated to exclusively use `process.env.API_KEY` and conform to `new GoogleGenAI({ apiKey: ... })` initialization.
const getGoogleAI = () => {
    const keyToUse = process.env.API_KEY;
    if (!keyToUse) {
        throw new Error("Gemini API key is not available. Please ensure process.env.API_KEY is set.");
    }
    return new GoogleGenAI({ apiKey: keyToUse });
};

const analysisSchema = {
    type: Type.OBJECT,
    properties: {
        query_summary: { type: Type.STRING, description: "A 1-2 sentence objective summary of the user's log." },
        emotions: {
            type: Type.OBJECT,
            properties: {
                joy: { type: Type.NUMBER }, sadness: { type: Type.NUMBER }, outburst: { type: Type.NUMBER },
                irritable: { type: Type.NUMBER }, timid: { type: Type.NUMBER }, anxiety: { type: Type.NUMBER },
                flustered: { type: Type.NUMBER }, envy: { type: Type.NUMBER }, boredom: { type: Type.NUMBER },
                exhaustion: { type: Type.NUMBER },
            },
            required: ['joy', 'sadness', 'outburst', 'irritable', 'timid', 'anxiety', 'flustered', 'envy', 'boredom', 'exhaustion'],
        },
        xp: { type: Type.NUMBER, description: "Experience Points (XP) between 5 and 25." },
    },
    required: ['query_summary', 'emotions', 'xp'],
};

// FIX: Removed apiKey parameter to use the centralized `getGoogleAI` function.
export async function analyzeLog(log: string): Promise<LogAnalysis> {
  // ìºì‹œ í™•ì¸
  const cached = conversationCache.get(log);
  if (cached) {
    console.log('âœ… Using cached conversation analysis');
    return {
      query_summary: cached.summary,
      emotions: cached.emotions as any,
      xp: cached.xp
    };
  }

  try {
    const startTime = performance.now();
    const ai = getGoogleAI();
    const prompt = `You are an emotion analysis AI for the A. me system. Analyze the user's log entry and provide a summary, calculate Experience Points (XP), and rate 10 emotions on a scale of 0.0 to 10.0.

User Log: "${log}"

RULES:
- XP should be between 5 and 25, based on the length and emotional depth of the log.
- Your response MUST be a valid JSON object following the provided schema.`;

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: prompt,
        config: { responseMimeType: 'application/json', responseSchema: analysisSchema },
    });
    
    const result = JSON.parse(response.text.trim());
    
    // ìºì‹œ ì €ì¥
    conversationCache.set(log, result.query_summary, result.emotions, result.xp);
    
    // ì„±ëŠ¥ ì¶”ì 
    const duration = performance.now() - startTime;
    console.log(`ğŸ“Š Analysis took ${duration.toFixed(0)}ms`);
    
    return result;
  } catch (error) {
    console.error("Error analyzing log:", error);
    throw new Error("Failed to analyze the log entry.");
  }
}

/**
 * í« ì´ë¯¸ì§€ ìƒì„± (ë ˆë²¨ì—…, ê°ì • ë³€í™” ë“±)
 * @param prompt - ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
 * @param baseImage - ê¸°ì¡´ ì´ë¯¸ì§€ (ì—°ì†ì„± ìœ ì§€ìš©)
 * @param emotion - ê°ì • (ìºì‹±ìš©)
 * @param level - ë ˆë²¨ (ìºì‹±ìš©)
 * @param useCache - ìºì‹œ ì‚¬ìš© ì—¬ë¶€
 * @returns Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ URL
 */
export async function generatePetImage(
    prompt: string,
    baseImage: {inlineData: {data:string, mimeType: string}} | null = null,
    emotion?: string,
    level?: number,
    useCache: boolean = true
): Promise<string> {
    // í…Œë§ˆ ê°€ì ¸ì˜¤ê¸°
    const theme = skinSettings.getSettings().theme;
    
    // ìºì‹œ í™•ì¸ (ë ˆë²¨ì—… ì´ë¯¸ì§€ë§Œ ìºì‹±)
    if (useCache && emotion && level) {
        const cached = await imageCache.get(emotion, level, theme);
        if (cached) {
            console.log('âœ… Using cached image');
            return cached;
        }
    }

    try {
        const startTime = performance.now();
        const ai = getGoogleAI();
        const parts: any[] = [{ text: prompt }];
        if (baseImage) parts.unshift(baseImage);

        const response = await ai.models.generateContent({
            model: 'gemini-2.5-flash-image',
            contents: { parts },
            config: { responseModalities: [Modality.IMAGE] },
        });

        const imagePart = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData);
        if (imagePart?.inlineData) {
            const imageUrl = `data:${imagePart.inlineData.mimeType};base64,${imagePart.inlineData.data}`;
            
            // ìºì‹œ ì €ì¥
            if (useCache && emotion && level) {
                await imageCache.set(emotion, level, imageUrl, theme);
            }
            
            const duration = performance.now() - startTime;
            console.log(`ğŸ¨ Image generation took ${duration.toFixed(0)}ms`);
            
            return imageUrl;
        }
        throw new Error("No image data found in response");
    } catch (error) {
        console.error("Error generating pet image:", error);
        throw new Error("Failed to generate a new image for the pet.");
    }
}

/**
 * ë ˆë²¨ì—…ì‹œ ì´ë²¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (í…Œë§ˆ ì ìš©)
 */
export async function generateLevelUpImage(
    petType: 'hatchi',
    level: number,
    emotion: string,
    levelName: string,
    baseImage: {inlineData: {data:string, mimeType: string}} | null = null
): Promise<string> {
    const theme = skinSettings.getSettings().theme;
    const prompt = petSkinGenerator.generateLevelUpPrompt(level, theme);
    return generatePetImage(prompt, baseImage, emotion, level, true);
}

/**
 * ê°ì • ê¸°ë°˜ í‘œì • ë³€í™” ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì‹œê°„ ëŒ€í™”ìš©, í…Œë§ˆ ì ìš©)
 */
export async function generateEmotionExpression(
    petType: 'hatchi',
    emotion: string,
    intensity: number,
    baseImage: {inlineData: {data:string, mimeType: string}} | null = null
): Promise<string> {
    const theme = skinSettings.getSettings().theme;
    const prompt = petSkinGenerator.generateExpressionPrompt(emotion, intensity, theme);
    return generatePetImage(prompt, baseImage, emotion, undefined, false); // ì‹¤ì‹œê°„ í‘œì •ì€ ìºì‹± ì•ˆí•¨
}

/**
 * ëŒ€í™” ì¤‘ ê°ì • ë¶„ì„ í›„ ì‹¤ì‹œê°„ í‘œì • ì—…ë°ì´íŠ¸
 * (Nano Banana ìŠ¤íƒ€ì¼ - ë¯¸ì„¸í•œ ë³€í™”, í…Œë§ˆ ì ìš©)
 */
export async function updateLiveExpression(
    currentImageUrl: string | null,
    emotion: string,
    intensity: number,
    petType: 'hatchi' = 'hatchi'
): Promise<string | null> {
    try {
        if (!currentImageUrl || !currentImageUrl.startsWith('data:image')) {
            return null;
        }

        const [header, data] = currentImageUrl.split(',');
        const mimeType = header.match(/:(.*?);/)?.[1];
        
        if (!data || !mimeType) {
            return null;
        }

        const baseImage = { inlineData: { data, mimeType } };
        
        // í…Œë§ˆ ì ìš© í”„ë¡¬í”„íŠ¸
        const theme = skinSettings.getSettings().theme;
        const prompt = petSkinGenerator.generateExpressionPrompt(emotion, intensity, theme);
        const updatedPrompt = `${prompt}\n\nIMPORTANT: Make only subtle changes to the facial expression. Keep the overall character design, colors, and style identical. Only adjust eyes, mouth, and minor emotional details.`;
        
        return await generatePetImage(updatedPrompt, baseImage, undefined, undefined, false);
    } catch (error) {
        console.error('Failed to update live expression:', error);
        return null; // ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ì´ë¯¸ì§€ ìœ ì§€
    }
}

async function* streamFromApi(endpoint: string, options: RequestInit, responseParser: (json: any) => string | null): AsyncGenerator<string> {
    try {
        const response = await fetch(endpoint, options);
        if (!response.ok) {
            const errorBody = await response.text();
            throw new Error(`API error (${response.status}): ${errorBody}`);
        }
        if (!response.body) throw new Error("No response body");

        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                if (line.trim().startsWith('data: ')) {
                    const jsonStr = line.substring(6);
                    if (jsonStr.trim() === '[DONE]') return;
                    try {
                        const parsed = JSON.parse(jsonStr);
                        const content = responseParser(parsed);
                        if (content) yield content;
                    } catch (e) {
                        console.error("Error parsing stream chunk:", jsonStr, e);
                    }
                }
            }
        }
    } catch (error) {
        console.error(`Error streaming from ${endpoint}:`, error);
        yield `Error: ${error instanceof Error ? error.message : "Unknown error"}`;
    }
}

async function* streamFromOpenRouter(modelId: string, history: Message[], newPrompt: string, apiKey: string): AsyncGenerator<string> {
    const messages = history
        .filter(msg => msg.role === 'user' || msg.role === 'model')
        .map(msg => ({ role: msg.role === 'model' ? 'assistant' : 'user', content: msg.content }));
    messages.push({ role: 'user', content: newPrompt });

    yield* streamFromApi(
        "https://openrouter.ai/api/v1/chat/completions",
        {
            method: "POST",
            headers: { "Authorization": `Bearer ${apiKey}`, "Content-Type": "application/json" },
            body: JSON.stringify({ model: modelId.replace('openrouter/', ''), messages, stream: true }),
        },
        (p) => p.choices?.[0]?.delta?.content
    );
}

async function* streamFromOpenAI(modelId: string, history: Message[], newPrompt: string, apiKey: string): AsyncGenerator<string> {
    const messages = history
        .filter(msg => msg.role === 'user' || msg.role === 'model')
        .map(msg => ({ role: msg.role === 'model' ? 'assistant' : 'user', content: msg.content }));
    messages.push({ role: 'user', content: newPrompt });
    
    yield* streamFromApi(
        "https://api.openai.com/v1/chat/completions",
        {
            method: "POST",
            headers: { "Authorization": `Bearer ${apiKey}`, "Content-Type": "application/json" },
            body: JSON.stringify({ model: modelId, messages, stream: true }),
        },
        (p) => p.choices?.[0]?.delta?.content
    );
}

async function* streamFromAnthropic(modelId: string, history: Message[], newPrompt: string, apiKey: string): AsyncGenerator<string> {
    const messages = history
        .filter(msg => msg.role === 'user' || msg.role === 'model')
        .map(msg => ({ role: msg.role, content: msg.content }));
    messages.push({ role: 'user', content: newPrompt });

    try {
        const response = await fetch("https://api.anthropic.com/v1/messages", {
            method: "POST",
            headers: { "x-api-key": apiKey, "anthropic-version": "2023-06-01", "Content-Type": "application/json" },
            body: JSON.stringify({ model: modelId, messages, max_tokens: 4096, stream: true }),
        });
        if (!response.ok) {
            const errorBody = await response.text();
            throw new Error(`API error (${response.status}): ${errorBody}`);
        }
        if (!response.body) throw new Error("No response body");

        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const jsonStr = line.substring(6);
                    const parsed = JSON.parse(jsonStr);
                    if (parsed.type === 'content_block_delta') {
                        yield parsed.delta.text;
                    }
                }
            }
        }
    } catch (error) {
        console.error("Error streaming from Anthropic:", error);
        yield `Error: ${error instanceof Error ? error.message : "Unknown error"}`;
    }
}

// FIX: Removed apiKey parameter to use the centralized `getGoogleAI` function.
async function* streamFromGemini(
  modelId: string, 
  history: Message[], 
  newPrompt: string,
  systemInstruction?: string
): AsyncGenerator<string> {
    const ai = getGoogleAI();
    const geminiHistory = history
        .filter(msg => msg.role !== 'system')
        .map(msg => ({ role: msg.role, parts: [{ text: msg.content }] }));
    try {
        const config: any = { model: modelId, history: geminiHistory };
        if (systemInstruction) {
          config.systemInstruction = systemInstruction;
        }
        
        const chat = ai.chats.create(config);
        const result = await chat.sendMessageStream({ message: newPrompt });
        for await (const chunk of result) {
            if (chunk.text) yield chunk.text;
        }
    } catch (error) {
        console.error("Error generating response from Gemini:", error);
        yield "Sorry, I encountered an error communicating with the AI.";
    }
}

export async function* generateChatResponseStream(
  model: Model, 
  history: Message[], 
  newPrompt: string, 
  apiKeys: ApiKeys,
  petState?: PetState
): AsyncGenerator<string> {
    // í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    let systemPrompt: string | undefined;
    if (petState?.persona && model.provider === 'Google Gemini') {
      const recentContext = buildRecentContext(petState.logHistory);
      systemPrompt = buildSystemPrompt(petState.persona, recentContext);
      console.log('ğŸ§  Persona System Prompt ì ìš©:', systemPrompt.slice(0, 100) + '...');
    }

    switch (model.provider) {
        case 'Google Gemini':
            yield* streamFromGemini(model.id, history, newPrompt, systemPrompt);
            break;
        case 'OpenRouter':
            if (!apiKeys.openrouter) { yield "OpenRouter API key is missing."; return; }
            yield* streamFromOpenRouter(model.id, history, newPrompt, apiKeys.openrouter);
            break;
        case 'OpenAI':
            if (!apiKeys.openai) { yield "OpenAI API key is missing."; return; }
            yield* streamFromOpenAI(model.id, history, newPrompt, apiKeys.openai);
            break;
        case 'Anthropic':
            if (!apiKeys.anthropic) { yield "Anthropic API key is missing."; return; }
            yield* streamFromAnthropic(model.id, history, newPrompt, apiKeys.anthropic);
            break;
        default:
            yield `Provider "${model.provider}" is not supported.`;
    }
}

// FIX: Removed apiKey parameter to use the centralized `getGoogleAI` function.
export async function* generateReflection(petState: PetState, question: string): AsyncGenerator<string> {
    const ai = getGoogleAI();
    
    // í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì„±ì°° í”„ë¡¬í”„íŠ¸ ìƒì„±
    const systemInstruction = buildReflectionPrompt(petState.persona, petState);
    console.log('ğŸ§˜ Reflection Prompt ìƒì„± ì™„ë£Œ');
    
    const myContext = `ìµœê·¼ ìš°ë¦¬ì˜ ëŒ€í™”ë¥¼ ëŒì•„ë³´ë©´ì„œ ë‹¹ì‹ ê»˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì–´ìš”.`;
    const prompt = `${myContext}\n\në‹¹ì‹ ì´ ë¬»ëŠ” ê²ƒ: "${question}"`;

    const stream = await ai.models.generateContentStream({
        model: 'gemini-2.5-pro',
        contents: prompt,
        config: { systemInstruction }
    });

    for await (const chunk of stream) {
        if (chunk.text) yield chunk.text;
    }
}

/**
 * í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ - 10íšŒ ëŒ€í™”ë§ˆë‹¤ LLMì—ê²Œ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ìš”ì²­
 */
export async function updatePersona(petState: PetState): Promise<PetPersona> {
    const ai = getGoogleAI();
    const recentLogs = getRecentLogs(petState.logHistory, 10);
    const prompt = buildPersonaSummaryPrompt(recentLogs, petState.persona);
    
    console.log('ğŸ”„ í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸ ì‹œì‘... (ìµœê·¼ 10ê°œ ëŒ€í™” ë¶„ì„)');
    
    try {
        const result = await ai.models.generateContent({
            model: 'gemini-2.5-flash',
            contents: prompt,
            config: { 
                responseMimeType: 'application/json',
                temperature: 0.7
            }
        });
        
        const responseText = result.text.trim();
        console.log('ğŸ“Š LLM ì‘ë‹µ:', responseText.slice(0, 200) + '...');
        
        // JSON íŒŒì‹±
        const summary = JSON.parse(responseText);
        
        // í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸
        const updatedPersona: PetPersona = {
            ...petState.persona,
            growthSummary: summary.growthSummary || petState.persona.growthSummary,
            userInsight: summary.userInsight || petState.persona.userInsight,
            coreTraits: summary.newTraits && summary.newTraits.length > 0 
                ? summary.newTraits 
                : petState.persona.coreTraits,
            emotionalProfile: calculateAverageEmotions(recentLogs),
            conversationCount: petState.persona.conversationCount,
            lastUpdated: new Date().toISOString()
        };
        
        console.log('âœ… í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ');
        console.log('  - ìƒˆ íŠ¹ì„±:', updatedPersona.coreTraits);
        console.log('  - ì„±ì¥ ìš”ì•½:', updatedPersona.growthSummary.slice(0, 50) + '...');
        
        return updatedPersona;
    } catch (error) {
        console.error('âŒ í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', error);
        // ì‹¤íŒ¨ì‹œ ê¸°ì¡´ í˜ë¥´ì†Œë‚˜ ìœ ì§€ (ë‹¨, ê°ì • í”„ë¡œí•„ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì—…ë°ì´íŠ¸)
        return {
            ...petState.persona,
            emotionalProfile: calculateAverageEmotions(recentLogs),
            lastUpdated: new Date().toISOString()
        };
    }
}